Week 1:

ML 
- Grew out of work in AI
- new capability for computers, machine learn to do things itself



Supervised Learning: Proabably the most common type of ML
"right answers" are given with data set; exsisting known answers are given within the dataset, what we gonna do is to decide the answers of some new data

Regression Problem: Pridict continuous valued output (price); pridict the continuous valued attributes

Classification Problem: Decrete valued output(e.g. 0 or 1 or 2, Y or N); unlike regression problems

algorithm: Support Vector


Unsupervised Learning: Here's the dataset, can you find some structure of the data?
e.g. Cocktail Party Problem Algorithm

Octave programming environment: free open source
http://octave.sourceforge.net/


materials:
https://share.coursera.org/wiki/index.php/ML:Main
https://www.khanacademy.org/#linear-algebra
https://en.wikipedia.org/wiki/Linear_least_squares_%28mathematics%29#Derivation_of_the_normal_equations

Model and Cost Function:
Regression Problem: pridict the outcome of real-value
Training Set
m: number of training examples
x's: "input"s
y's: "output"s, targets
(x, y): a training example
(x(i), y(i)): the i-th training example
Training Set -(feed to)-> Learning Algorithm -(out put a function)-> h(denoted as h, stands for Hypothesis; takes in new inputs and then will be able to estimate the output; maps from x's to y's)
How do we represent h? h(x) / hΘ(0) (theta)

Linear Regression with one variable: Univariate linear regression

Cost function: how to discover the best-fit solution of the data
linear regression:
Hypothesis: hΘ(x) = Θ0 + Θ1x
Θi's :(Θ0, Θ1) parameters of the model
The square error function
The goal is to minimize the cost function J(Θ0, Θ1, ...)

Coutour Plots

Parameter Learning:
Gradient Descent: starting at some point and "take a small step toward which direction will I go 'down' as quickly as possible?"
":=" : assignment operator
"=" : truth assertion
alpha: "learning rate", how big the step we take
correct implementation: // simutaneusly update
tmp1 := GD(Θ0) // GD adapt the value according to the deriviate at point (Θ0, Θ1)
tmp2 := GD(Θ1)
Θ := tmp1
Θ := tmp2
if alpha is too large, we can fail to converge, or even diverge
already at a local minimum: unchange
alpha could be fixed, no need to decrease, because the "step" is meant to be smaller and smaller
step depends on the derivatives of J(Θ0, Θ1)

Convex Function: like a linear regression; always a "bowl-shape"

"Batch" Gradient Descent: each step uses all the training examples 

Normal Equation Method: no need to ML; but with larger steps, GD is better



Linear Algebra Review:

Matrix: dimension; written as number of rows X number of colums (dimension is R by C); A12 for the row 1 col 2 of matrix A
Vector: a N X 1 matrix
R 3X2: all aD M with 3 X 2
R 4: all vectors with 4 row
1-index vector (index start from 1) and 0-index vector

Addition and Scalar Multiplication:
addition / subtraction: corresponding numbers add to / subtract from each other
scalar: scalar number is a real number that apply to everything in the matrix accordingly

Matrix Vector Multiplication: 
Mab X Vb = Va: Ri = ∑Mij * Vj
Normally we say, A X x = y
y = Θ0 + Θ1 * x
=>
x: m * 2, xi1 = 1, xi2 = data
A: 2 * 1, a1 = Θ0, a2 = Θ1
Prediction = Data * Parameter

Matrix Matrix Multiplication:
Xab * Ybc = Zac
Zij = ∑Xik * Ykj

Usage: comparing different hypothesis by making the matrix A's each col a solution of (Θ0, Θ1)； outcome will be the prediction

Matrix Multiplication Properties:
not commutative (A X B != B X A)
associative (A X B X C = A X (B X C))
Identity Matrix: I (Iij = 0 if i != j, = 1 if i == j); AI = IA = A

Inverse and Transpose:
Inverse: 
A is a m X m matrix (square matrix), if it has an inverse A(-1); A*A(-1) = I
only square matrix have inverse
Transpose: row index <--> col index

octave:
A = [1 2; 3 4] (1, 2) is what inside the first row
pinv(A) : inverse
A * B: mul
matrix who don't have an inverse: singular matrix / degenerate matrix




Week 2:

Matlab Online!
https://matlab.mathworks.com/

Multiple Features (variables):
n : number of features
x(i): input features of the i-th input (training example)
xj(i): the j-th feature of x(i)
Hypothesis: hΘ(x) = Θ0 + Θ1x1 + Θ2x2 + ...
for convenience: x0 = 1 (additional x0 feature)
define vector Θ and vector X
Θ(^T)X : hΘ(x)
cost function: J(Θ) = J(Θ0, Θ1, ...) = 1/2m * alpha * ∑(i = 1~m)(hΘ(x(i)) - y(i))^2
repeat: Θj := Θj - ∂J(Θ)/∂Θj
Θj := Θj - ∂J(Θ)/∂Θj = Θj - 1/m * alpha * ∑(i = 1~m)(hΘ(x(i)) - y(i)) * xj(i)

Feature Scaling:
Feature scaling speeds up gradient descent by avoiding many extra iterations that are required when one or more features take on much larger values than the rest.
improper scaling might lead to too much troubles: we can get every feature into an approximately [-1, 1] range (-1, 1 could be changed, 2, 3, 0 would be fine, but like 100, would be too large to be proper, so as 0.001 poorly schaled)

mean normalization: 
replace xi with xi - μi (do not apply to x0 = 1): μi is the everage of xi in the training set
make features have approximately zero mean
and divide (xi - μi) with Si: Si is the range of value, (max - min; standared deviation would be fine, too)

If GD works correctly, J should decrease after each iteration
if not working, choose a smaller alpha; but if alpha is too small, it'll be slow

Features and Polynomial Regression:

If the features are depending on each other, we can redefine new variables to improve the model
cubic model could be represented just like the linear regression model, but x1 = a, x2 = a^2, x3 = a^3; if you do so, feature scaling would be extremely important so that they'll be at approximately same range; for example, without mean normalization, we could let it be xi = func(ai) / func(max - min)

Computing Parameters Analytically:

Normal Equation V.S. GD

intuition: the simple case is to solve the function and make the derivative of theta becomes zero
X: M(m x (n + 1))
y: M(m-d vector)
Θ = (((x^T)X)^(-1))(X^T)y
// Octave: pinv(x' * x) * x' * y

Normal Equation: no need to do feature scaling; otherwise (GD) feature scaling is important

GD: 
have to choose alpha, have to iterate
work well when n is large
universally useful
NE: 
no need of alpha, no need to iterate
have to calculate (x' * x) so become really slow when x grows large O(n^3) large(approximately) : thousand
couldn't apply to all 


Assignment ref:
https://www.coursera.org/learn/machine-learning/discussions/vgCyrQoMEeWv5yIAC00Eog
http://octave.sourceforge.net/octave/overview.html
answer:
http://www.cnblogs.com/CheeseZH/p/4597006.html

% A = [16 2 3 13; 5 11 10 8; 9 7 6 12; 4 14 15 1]
% A = [1 2 3; 4 5 6; 7 8 9]

Octave Tutorial

== : equals
~= : not equal
xor(1, 0)
PS1(">> ") : change the appearance of the CLI
c = (3 > 1) // then c will be 1 
s = "str" // then s will be a string with content "str"
a = pi // then a will be 3.1416 
disp(a) will print a
disp(sprintf('content text %0.2f', a)) will show "content text 3.14"
// mixed with old-style C
add a semicolon and the command line wouldn't print out a thing
A = [1 2; 3 4] // ; == go to the next row
equivalent to:
>> A = [1 2;
>> 3 4]
if we do this: v = a:b:c
then v = [a a+b a+2*b ...... c]
v = 1:5
then v = 1 2 3 4 5
ans = ones(2, 3) then ans = [1 1 1; 1 1 1]
zeros(1, 3) then [0 0 0]
rand(2, 3) then all random numbers(0~1) in a 2 X 3 array
randn(x, x) then normal random (around 0)
sqrt(a) = square root
hist(w) %plot
hist(w, 50) %plot with 50 bins
eye(x): x X x matrix of I
help: help eye / help rand / help help

moving data around:
size(A): returns a 1 * 2 matrix (n_row, n_col)
size(A, 1): n_rows in A
size(A, 2): n_cols in A
length(A): return the longer one in n_col and n_row of A

pwd: returns the path; and could use cd / ls as cmd
who: variables already in the memory of current scope
whos: details

load readme.dat / load('readme.dat')

clear A % clear it from memory

v = vectorY(1:10) % first 10 elements of vectorY

clear % clear all

save hello.txt v -ascii % save a human readable file

A(3, 2) % A_32
A(3, :) % everything in the 3rd row
A(:, 2) % everything in the 2nd col
A([1 3], :) % from row 1 and 3 and all cols

the above expressions could also be used to assign variable values

A = 
1 2
3 4
5 6
A = [A, [7; 8; 9]] % append another col vector to the right
then A=
1 2 7
3 4 8
5 6 9

A(:) put all elem in A into a single vector (by col)

C = [A B] % concatenating matrix A and B onto each other (A on the left, B on the right)

C = [A; B] % A on top, B on the bottom

C = A .^ B; % C(i, j) = A(i, j) * B(i, j); could be also implemented as A .^2 if A == B

B = 1 ./ A % u(i) = 1 / A(i, j)

others: log(v), exp(v) /* e^v */, abs /* absolute value */, +1 / + ones( , ), sum(A) /* sum of all elements */, prod(A) /* product of all elements */, floor(A) /* rounds down */, ceil(A) /* rounds up */; flipud(A) /* turn upside down */

val = max(A) ; [val, index] = max(A)

A < 3: boolean matrix
find(A < 3): return the indexx

magic(3): a 3-by-3 special matrix; each row / col/diag has the same sum

max(A, B) % pick the bigger one of corresponding position of A & B
max(A, [], 1) % max of each col
max(A, [], 2) % max of each row
max(A) max of each col; max(max(A)) /* the real max */

sum(A, 1) % sum each col
sum(A, 2) % sum each row

flipud(A) /* turn upside down */
pinv(A) % inverse of A

Plotting (Visualizing) Data
y1 = function1(x)
y2 = function2(x)
plot(x, y1)
plot(x, y2)
if want to show them together, plot(x, y1); hold on; plot(x, y2)
% hold on: no new graphs
plot(x, y, 'r')  % r: event color
xlabel('x-name')
ylabel('y-name')
legend('sin', 'cos') % show legend to indicate which is what
title('my plot') % want a title
print -dpng "mywork.png" % print the result out
help plot / help print % could help
close; % close the plot
figure(1); plot(...);
figure(2); plot(...); % open two figures
subplot: make multiple plots on one figure
axis([ 1 2 3 4 5]) % change the x indexs
clf; % clear the last figure

visualization:
imagesc(A)
imagesc(A), colorbar, colormap gray;

statement 1, statement 2, statement 3 % comma chaining / chaining of commands

Control Statements: for, while, if statement
for i = 1:10, % for (i = 1; i <= 10; i++)
    statement1;
end; % could use break / continue; could also assign 1:10 to j then for i = j
i = 1;
while i <= 10,
    statement1;
    i = i+1;
end;
if i == 1,
    st1;
elseif i == 2,
    st2;
else
    st3;
end;

in Octave function name should be the file name (.m) // and the first line is function y = FunctionName(x) // y is the return value, when multiple, do [y1, y2]
and functions ould be directly used in cmd
addpath('C:\.......') makes it possible to acceess files in different directories

Vectorization: make your program runs faster

use libraries: more efficient, less coding // also similar libraries and functions in C++ or so
use vectors and existing functions rather than implement the process yourself
// P.S. Octave index start from 1, not 0 by default

pinv: invert even if X is non-invertable
inv: real invert

causes to be non-invertable: 
- redundant data
- too many features (delete some or do regularization)

Week3: 
use proxy to submit
ZYQLHaWEqxfLqeaw

Classification and Representation:

Classification:
y = 0 or 1 (negative class / positive class)
later: multi-class classification (0, 1, 2, 3, etc)
linear regression isn't a good solution to classification problem

logistic regression: hΘ always in the range [0, 1]
Hypothesis Representation:
linear: hΘ(x) = Θ' * x
now: hΘ(x) = g(Θ' * x)
g is sigmoid function / logistic function
g(z) = 1/(1 + e^(-z))
hΘ(x): means the possibility that y = 1 on input x; normally if it is >= 0.5 then we assume y to be 1 (that is z >= 0)

Decision Boundary: the line we draw that devide the different categories; might be non-linear----then add xi^2 (x1^2, x2^2), xi^3, etc.

convex: guarantee to converge to the global minimum
J = (1 / m) * ∑(Cost(h(x), y))
so, cost function would be: 
Cost(hΘ(x), y) =
-log(hΘ(x)) (if y = 1)
-log(1 - hΘ(x)) (if y = 0)
an equivalent way: Cost(hΘ(x), y) = -y * log(hΘ(x))-(1 - y) * log(1 - hΘ(x))
// the principle of the maximum lkelihood estimation
// still using Θj = Θj - alpha*derivative to update
// hapen to be the same as linear regression sum((h(x) - y)x) * alpha

Advanced Optimization (other than gradient descent: faster, scalable; no need to pick alpha; but more complex)
e.g.
conjugate gradient
BFGS
L-BFGS

use libraries; good libraries and bad libraries are very different, try and choose good ones
in octave:
fminunc

parameter could be function (start with @)


Multiclass Classification: One-vs-all (/ one-vs-rest)

make a prediction by choose the greatest hΘ^i(x) and decide that the class is i

http://www.68idc.cn/help/mysqldata/DB2/20150807486162.html
http://www.cnblogs.com/CheeseZH/p/4600837.html
http://blog.csdn.net/yunlong34574/article/details/8928089



Solving the Problem of Overfitting

regularization

under fit = high bias
over fit = high variance => too many features and not many training examples; fit the training set but fail on new examples

solution:
1. reduce number of features
-- Manually select
-- Model Selection Algorithm
2. Regularization
-- Keep all the features but reduce the magnitude / value of parameters Θ; work well with many parameters

Regularization:
try to give simpler hypothesis by making Θ3, Θ4, etc. close to zero; less prone to overfitting
J plus (1 / 2m) * λ(∑(Θ^2)) // Θ0 in linear regression should not be treated here !!!
λ: regularization parameter

each iteration of Θ:
Θj minus alpha*(λ/m)*Θj // linear
Normal Equation:
X: (m * (n + 1))
A: (n + 1) * (n + 1), looks like I (/ E), except that A11 = 0
y: R^m
Θ = (((X^T)X)^(-1))(X^T)y
=>
Θ = (((X^T)X + λ*A)^(-1))(X^T)y
// WON'T be non-invertable

Logistic Regression:
add to J: (λ / 2m) * ∑(Θ^2) // still, Θ0 won't be included
Θj minus alpha*(λ/m)*Θj // each step



Week 4:
Neural Networks: Representation

motivation:
Non-linear Hypotheses:
multiple features -> all the quadratic terms: would be too many of them; inefficient (N too large)
Origin: try to mimic the brain

The "one learning algorithm" hypothesis: cortex learn different thing if you modify it; neuro-rewrite experiment; sensor signal -> brain learn how to deal with them

Neural Networks:

Model Representation:

natural neuron: Dendrite "input wires"; "Axon" output wires
neuron model: logistic unit

x0: bias unit / bias neuron (always 1)

Sigmoid function = logistic activation function

Θ: weight / parameter (Θ(j) matrix of weight controling function mapping from layer j to layer j+1)

layer 1: (x0,) x1, x2 (original input)(input layer)
layer 2: (a0,) a1, ... (not input not output: hidden layer) (a_i(j): "activation of a(i) in layer j")
final layer: output layer
if there are s(j) units in layer j and s(j + 1) units in layer j + 1 then Θj will be a s(j + 1) * (s(j) + 1) matrix

a_i(2) = g(∑(Θ_ij(1)xj)) (i = 1 2 3, j = 0 1 2 3)
x = [x0; x1; x2; x3]
z(2) = [z_1(2); z_2(2); z_3(2)] // z_i(2) = ∑(Θ_ij(1)xj)
z(2) = Θ1 * x
a(2) = g(z(2))
a0(i) = 1 // not necessarily added
forward propagation

could be other architechtures

if a >= 4.6 g(a) = 0.99 (1)
if a <= -4.6 g(a) = 0.01 (0)

multi-class classification: multiple outputs (one 1, others 0 would be the best case)

ex3:
http://www.cnblogs.com/CheeseZH/p/4601911.html
http://blog.csdn.net/garfielder007/article/details/49979563
http://blog.csdn.net/hgaolbb/article/details/44678291
http://octave.sourceforge.net/docs.html



Week 5:

Lesson 1: Cost Function and Backpropagation

Classification:
data: {(x^(1), y^(1)), ...}
L: total number of layers
S_l: total number of units (not counting bias units) in layer l
binary classification: 1 output unit (y = 0 or 1)
multi-class classification(K classes): K output units [0, 0, ... 1, 0]

Cost Function:

for logistic regression: 
J(Θ) = -1/m * [∑(i = 1~m)y^(i) * log(hΘ(x^(i)) + (1 - y^(i)) * log(1 - hΘ(x^(i)))] + lambda/(2*m) * ∑(j = 1~n) (Θ_j)^2
// not counting in Θ0

Neural Network:
(hΘ(x))_i = ith output
L layers, K outputs
J(Θ) = -1/m * [∑(i = 1~m) * ∑(k = 1~K) * y_k^(i) * log(hΘ(x^(i))_k + (1 - y^(i)_k) * log(1 - hΘ(x^(i))_k)] + lambda/(2*m) * ∑(l = 1~L-1)∑(i = 1~s_l)∑(j = 1~s_(l+1)) ((Θ_ji)^(l))^2

Backpropagation Algorithm: used to minimize the cost function
a(1) = x
z(2) = Θ(1)a(1)
a(2) = g(z(2)) // add a_0(2)
z(3) = Θ(2)a(2)
a(3) = g(z(3)) // add a_0(3)
Gradient Computation: Backpropagation Algorithm
δ_j^(l): error of node j in layer l
// assume L = 4 then
δ_j^(4) = a_j^(4) - y_j
δ^(3) = ( (Θ^(3))^T * δ^(4) ) .* g'(z^(3)) // a(3) .* (1-a(3))
so does δ^(2)
if lambda = 0 then derievative of J(Θ) according to Θ_j^(l) = a_j(l) * δ_j(l + 1)
Backpropagation Algorithm:
Set Δij(l) = 0 (for all i, j, l)
For i = 1 to m:
	Set a(1) = x
	Perform forward propagation to computer a(l) for l = 2, 3, ... L
	Compute δ(L) using y(i)
	Compute δ(L - 1) to δ(2)
	Δij(l) = Δij(l) + a_j(l) * δ_i(l + 1)
	// that is to say:
	// Δ(l) = Δ(l) + a_j(l) * δ_i(l + 1)
	If j != 0 then:
		Dij(l) = 1/m * Δij(l) + λ * Θij(l) // λ is lambda
	Else:
		Dij(l) = 1/m * Δij(l)
	// Dij(l) is the deriviative of J(Θ) by Θij(l)
// {(x1, y1), (x2, y2)} in this order: FP using x1, BP using y1, FP using x2, BP using y2 // FP = forward propagation; BP = backward propagation

if only one output layer:
δj(l): error of cost of aj(l) // unit j in layer l
cost(i) = y(i)log(hΘ(x(i))) + (1- y(i))log(hΘ(x(i)))
Formally, δj(l) is the derievative of cost(i) according to zj(l)

Implementation Note: Unrolling Parameters
DVec = [D1(:); D2(:); D3(:)]
reshape(DVec(1:110), 10, 11)
unroll to pass parameters into fminunc

Gradient Checking:
FP & BP: bug-checking

Random Initialization: symmetry breaking
because zero initialization for Θ isn't useful in neural network
 
Putting It Together:
default: 1 hidden layer; or if hidden layer > 1, same number of units in each layer (and usually, the more the better)
Training a Neural Network:
1. randomly initialize weight
2. implement forward propagation to get hΘ(x(i)) for any x(i)
3. compute cost function J(Θ)
4. implement backward propagation to get partial derivatives (Θjk(l) to J(Θ))
5. gradient check: compare the result (partial derivative) with J(Θ)'s gradient // Then disable GC code, for it'll be otherwise really slow
6. minimize J(Θ) (using gradient descent or advanced optimization)

EX4:
https://github.com/vgoklani/Stanford-Machine-Learning
http://www.cnblogs.com/CheeseZH/p/4608129.html

Example: Autonomous Driving



Week 6: Advice for Applying Machine Learning & Machine Learning System Design

Lesson 1: Evaluating a Learning Algorithm

Debugging a learning algorithm:
- get more training examples
- try smaller set of features
- try getting additional features
- try adding polynomial features
- try decreasing λ
- try increasing λ

Machine Learning Diagnostic: A test you can run to gain insight what is/ isn't work with learning algorithm, and gain guidiance as to how best to improve its performance
Diagnostics might spend time to build, but it's worthwhile

Evaluating a Hypothesis: how to evaluate it?
1. randomly split the dataset into two portions: training set & test set (typically 7:3)
2. learn from training set minimizing J(Θ)
3. compute test set error J_test(Θ) // square error; compute test set error with Θ given; misclassification error, etc.

Model Selection and Train/Validation/Test Sets

Model Selection problem: x?x^2?x^3? ...
calculate the parameter Θ for each of the alternatives and then compare test error.

we can devide the dataset into three parts:
training set:
cross validation set:
test set:
// typically 6:2:2

training error, cross validation error (J_cv(Θ)) and test error are quite similarly calculated
usage: training set->get Θ, but measure the hypothesis with the lowest J_cv(Θ); save away the test set for evaluation (could be evalated on test set!)


Bias vs. Variance

Diagnosing Bias vs. Variance
high bias: underfit; J_train and J_cv would both be high
high variance: overfit; J_train low, J_cv >> J_train

Regularization and Bias/Variance:
large λ: underfit (high bias), J_train high, J_cv high
intermediate λ: just right, J_train okay, J_cv high
small λ: overfit (high variance), J_train low, J_cv high

How to decide λ?
J_train(Θ) no longer the same as J(Θ); instead we have the J_train(Θ) as J(Θ) with λ === 0
J_cv nd J_test are all the similar with J_train
try different alternatives of λ values to minimize J(Θ), and use the J_cv(Θ) to evaluate it
pick the best Θ according to cv and test on test set J_test

Learning Curves: an useful thing to plot
error - m (training set size)
plot J_train or J_cv
J_train would grow as m grows
J_cv would fall as m grows
if high bias: J_train would end up close to J_cv
if high variance: J_train grows really slow, J_cv would remain high; large gap

High bias: getting more training data would not help much by itself; usually not many parameters, usually large lambda
High validation: getting more training data is likely to help; usually too much parameters, usually small lambda

So what to do next?
- get more training examples		: for high variance
- try smaller set of features		: fixes high variance
- try getting additional features	: fixes high bias
- try adding polynomial features	: fixes high bias
- try decreasing λ					: fixes high bias
- try increasing λ					: fixes high variance

neural networks:
"small": underfit, simpler and cost less to calculate
"large": more prone to overfit, more parameters

ex5:
https://www.coursera.org/learn/machine-learning/discussions/vgCyrQoMEeWv5yIAC00Eog
when forgotten to replace lib, youshould re-logon to Octave
http://www.cnblogs.com/CheeseZH/p/4673488.html
https://github.com/vgoklani/Stanford-Machine-Learning/tree/master/linear-regression/mlclass-ex5


Lesson 2: Machine Learning System Design

Building a Spam Classifier

Prioritizing What to Work On:
supervised learning: x = features of the email (e.g. key words have 1 / not have 0)
How to make it has lower error?
- Collect lots of data (e.g. "honeypot" project)
- Develop sophisticated features based on email routing information (from email header)
- Develop sophisticated features for message body (e.g. deal and Dealer, features about punctuation)
- Develop sophisticated algorithm to detect misspelling

Error Analysis
Recommended approach:
- Start with simple algorithm that you can implement quickly, and test it on cross-validation data
- Plot curves to decide if more data & more features, etc. are likely to help
- Error Analysis: manually examine the examples in cv dataset where the algorithm made errors on; see if you plot any systematic trend in what type of examples it is making errors on // may be useless; should try and see if it works

the importance of numerical evaluation
// can use "stemming" software (e.g. "Porter stemmer") // might lead to worse performance

Error Metrics for Skewed Classes:
less error isn't necessaryly better performance e.g. always y = 0

precision / recall
true/false (true: actual == predicted) + positive/negative (positive: predicted y == 1, called skewed classes)
precision = true_positive / predicted_positive
recall = true_positive / actual_positive

high precision or high recall:=> a good classifier

usually y = 1 is used for rare class that we would like to predict, while 0 stands for the general cases

For reference:
Accuracy = (true positives + true negatives) / (total examples)
Precision = (true positives) / (true positives + false positives)
Recall = (true positives) / (true positives + false negatives)
F1 score = (2 * precision * recall) / (precision + recall)

Trading Off Precision and Recall:
predict y = 1 only if very confident: raise the threshold of h (rather than 0.5), higher precision but lower recall
other wise (avoid false negative): lower the threshold of h, high recall but low precision

F1 score (F score): help you decide which algorithm is better according to precision and recall values
F = 2 * PR/(P + R) // if P = R = 0 then F = 0
measure on CV set and choose the threshold with the highest F


Using Large Data Sets:
when getting a lot of data would help?
- useful test: given x, human expert could predict y; have sufficient information about the result to conclude
- use a large data set makes it unlikely to overfit; an algorithm who has many parameters, or any other overfitting/high variance cases would be helped by it



Week 7: Support Vector Machines

Large Margin Classification:

Optimization Objective:
by now: supervised learning

SVM: sometimes a more powerful way to learn non-linear functions

alternative way of logistic regression: use straight lines instead (make it simpler)
z = Θ^T * x
cost_0(z): when y = 0, cost-x plot (the line is replaced by similar straight version)
cost_1(z): when y = 1
J = C * [∑(i=1~m) (y^(i) * cost_1(Θ^T * x^(i)) + (1 - y^(i)) * cost_0(Θ^T * x^(i)))] + 1 / 2 * ∑(j=1~n) Θ_j^2 // C could be the same as 1 / lambda
goal: min(J)
h_Θ(x) = 1 if z >= 0 and 0 if z < 0

Large Margin Intuition:

people sometimes talk about SVM as Large Margin classifier

safty margin factor:
imagine the straightlines turn at -1, 1
if y = 1 we want z >= 1 (not just 0)
if y = 0 we want z <= -1 (not just 0)

if C is too large it'll fail to left out outliers properly, ending up in a bad solution

Mathematics Behind Large Margin Classification
Vector Inner Product:
||u|| = length of vector u; norm
p = the length of the projection of vector v onto vector u; // cos(<u,v>) * ||v||
u^T * v = p * ||u||
= v^T * u
= u1v1 + u2v2
1 / 2 * ∑(j=1~n) Θ_j^2: it is the length's square of Θ (Θ=[Θ_1; Θ_2; ...])
z = p(i) * ||Θ||; p(i) is the projection of x(i) onto the vector Θ
simplification: Θ_0 = 0 (decision boundary pass through the origin point for sure)
Θ applied to the parameters is not the Decision Boundary in the space; the direction of Θ is 90 degree with the real decision boundary

Kernels:
used for finding non-linear decision boundaries
define new features for SVM

Given x, compute new feature depending on proximity to landmarks l(1), l(2), l(3), etc.
f1 = similarity(x, l(1)) = exp(- (|| x - l(1) || ^2 / (2*σ^2)))
f2, f3 similar
=> fi are kernel functions, to be specific, these are Gaussian Kernels
also written as k(x, l(i))
if x is close to k(x, l(i)) then fi is approximately 1
if x is far away from it, fi would be close to 0
the larger the σ^2 is, the slower the "mountain" would fall, and the "fatter" would it be; the height and the position of the "top" are unchanged

predict 1 when Θ_0 + Θ_1 * f_1 + Θ_2 * f_2 + Θ_3 * f_3 >= 0

choosing landmarks for kernel:
exact the same position as the training examples l(1) is the same as x(1) // y not important here

for x^(i), f_j^(i) = sim(x^(i), l^(j))
f_i^(i) would be 1
new feature vector to represent my training examples: f^(i) = [f0(i); f1(i); ... fm(i)]

SVM with Kernels: // work perfectly together; kernel would cause other kind of algorithms to be slow
predict y = 1 when Θ^T * f >= 0
J = C * [∑(i=1~m) (y^(i) * cost_1(Θ^T * f^(i)) + (1 - y^(i)) * cost_0(Θ^T * f^(i)))] + 1 / 2 * ∑(j=1~n) Θ_j^2 // n = m
mathmatical detail: sometimes Θ^T * Θ is reshaped into Θ^T * M * Θ

SVM parameters:
C:
Large C = lower bias, high variance
Small C = higher bias, low variance
σ^2:
Large σ^2: feature fi vary more smoothly; Higher bias, lower variance
Small σ^2: feature fi vary less smoothly; Lower bias, higher variance
// make your choice based on the cross-validation set

Using An SVM:
use svm software packages: liblinear, libsvm, etc.
need to specify: C; Kernel (similarity function)

Kernel:
no kernel = linear kernel; suitable for n large and m small
Gaussian kernel: need to choose σ^2; n small m large
// note: do perform feature scaling before using Gaussian kernel: x_i and y_i for different (i)s are different

other kernels:
polynomial kernels: k(x, l) = (x^T * l + constant) ^ degree // suitable for non-negative x & l
string kernel, chi-square kernel, etc.

kernels should satisfy the technical condition: "Mercer's Theorem"

many SVM package already have multi-class classification built-in, otherwise we can use 1-v-n

logistic regression V.S. SVM:
if n small m medium: SVM with gaussian kernel
if n small m large: 1. add features; 2. SVM without kernel/ logistic regression
if n is large: SVM without kernel/ logistic regression

neural network: slower to train

https://github.com/ahawker/machine-learning-coursera/tree/master/ex6
https://github.com/vgoklani/Stanford-Machine-Learning/tree/master/support-vector-machine/mlclass-ex6



Week 8: Unsupervised Learning

Lesson 1: Clustering

the first unsupervised learning algorithm we'll learn: learn from unlabeled data instead of labeled data
with all x and no y, try find some structure


a specific algorithm in doing so is K-Means Algorithm
the most popular algorithm to -- automatically group the data into coherent subsets or into coherent clusters for us -- by far

e.g. group into 2 clusters:
1. randomly initialize two points called the Cluster Centroids
2. cluster assignment: for each point, choose one of the two CC to be grouped into (based on which is closer to)
3. move centroids: move the CC's to the avarage position of their "followers" respectively (move to new means)
4. re-group, run several iterations until no longer changing

CC is called μ_i (i from 1 to K)
c(i) is the closest μ's index (to x(i)) measured by square distance (i from 1 to m)

K-means for non-seperable datasets: will result in deviding the data into segments

Optimization Objective: --- cost function in K-means, also called a "Distortion"
will help with: debug, performance
J(c...μ...) = 1/m * ∑(i=1~m) (x(i) - μ_c(i))^2
The cost function should NEVER increase, unless there's bug in the code

Random Initialization:
initialize K-means and avoid local optima
given different initialization, there could be different outcomes (results)
μ(i) happens to be the same with x(j) at the begining: it is the RECOMMENDED way of initialization!
we can run it multiple times and choose the one with the least J(cost function)/lowest cost
when K is small, the influence of initialization is large

Choosing the number of clusters(K):
automatic method of choosing the right K: the Elbow method:
plot the graph: J-K (cost function of the result with different Ks)
choose the "elbow" point (where the decrease of J turns from fast to slow)
but there might end up with no "elbow"
but should be going down unless stuck in local optima when K = k_something
could take later/downstream purpose into consideration while selecting K


Lesson 2: Dimensionality Reduction --- a second type of unsupervised learning problem

Motivation:
Motivation I: Data Compression
e.g. reduce from 2D to 1D (a line) / 3D to 2D (a plain), 100D to ... etc.
could reduce redundancy (the attributes that are highly attributed could be appear only once)
we'll get a lower dimensional dataset {z(1), z(2), ... z(m)} instead of the original {x(1), x(2), ... x(m)}
Motivation II: Visualization
DR would help us visualize our data better
we are expecting to have a 2 or 3 dimensional dataset

Principal Component Analysis: (PCA)
by far the most popular algorithm of DR
minimize the "projection error" (the "blue line" segments in the vedio: project the data onto the line and minimize the "distance" between the "points" and the "line")
(feature scaling, normalization)
goal of PCA: find a vector u(1) onto which we could minimize the projection error
u(1) or -u(1) doesn't matter
it is at the same direction as the "line" we find (in 2D->1D case)
if not 2D->1D: n-D->k-D: find k vectors (u(1), u(2), ... u(k)) onto which to project the data; so as to minimize the projection error
e.g. 3D->2D: 2 vectors define a plane onto which we project the data

PCA and linear regression are totally different algorithms
linear regression: consider the vertical distance; trying to predict y;
PCA: projection distance; trying to predict a line and all parameters are treated equally;

Principal Component Analysis Algorithm:

Data Preprocessing:
Training set: x(1), x(2), ... x(m); 
Preprocessing (feature scaling/ mean normalization):
	μ_j = 1/m * ∑(i = 1~m) x_j(i) 
	Replace each x_j(i) with x_j(i) - μ_j
	If different features have different scales, scale them to have comparable range of values (e.g. (x_j(i) - μ_j)/s_j)
Reduce data from n-dimensions to k-dimensions:
	Compute "covariance matrix" (Σ / sigma):
		Σ = 1/m * ∑(i = 1~n) (x(i)) (x(i))^T // [n * n] = [n * 1] * [1 * n] // // X = [x(1); x(2); x(3) ...] then Sigma = 1/m * X' * X
	Compute "eigenvectors" of vector Σ:
		[U, S, V] = svd(Sigma) // singular value decomposition
	From [U, S, V] we get:
		U: [n * n], colum i is exactly u(i) we want; the first k cols are what we're looking for, together called U_reduce
			Z(i) = U_reduce^T * x(i) // [k * n] * [n * 1] = [k * 1]
		// PCA is difficult to prove but easy to implement
		S: [n * n], s_11, s_22, s_nn are only elements that are not 0, everything out the diagno would be 0;


Applying PCA:

Reconstruction from Compressed Representation:
PCA: a compressing algorithm (reduce dimension): THUS, should be a way to go back from its compressed representation back to an approximation of the original high dimensional data
since Z = U_reduce^T * X
X_approx = U_reduce * Z //[n*1] = [n*k]*[k*1]
this is a pretty decent approximation

Choosing the Number of Principal Components: (how to choose K?)
Choosing K:
	Average squared projection error: 1/m * ∑(i = 1~m) ||x(i) - x_approx(i)||^2 // a
	Total variation in the data: 1/m * ∑(i = 1~m) ||x(i)||^2 // b
	Typically, choose k to be smallest value so that:
		a / b <= 0.01 (1%) // 99% of variance is retained
		// maybe 5% and 95% variance retained,etc.

an algorithm:
	Start with try PCA with k = 1
	Compute U_reduce, z(1)~z(m), x_approx(1)~x_approx(m)
	Check if a/b <= 0.01
	k++ and re-run

[U, S, V] = svd(Sigma)
actually
a = 1/m * ∑(i = 1~m) ||x(i) - x_approx(i)||^2 
b =  1/m * ∑(i = 1~m) ||x(i)||^2 
a / b <= 0.01
a / b = 1 - ∑(i = 1~k) sii/ ∑(i = 1~n) Sii

thus we can compute [U, S, V] first
and then choose k by computing ∑(i = 1~k) sii/ ∑(i = 1~n) Sii


Advice for Applying PCA:
speed up learning algorithm

Compression:
Supervised learning algorithm speedup: (x(1), y(1)) ... (x(m), y(m))
	Extract input, do PCA and get new training set (z(i), y(i)) (i=1~m) Note: the PCA mapping should be defined by running PCA on the training set, and it can be applied as well to x_cv and x_test
Visualization:
Reduce k to 2 or 3 so as to plot the data (otherwise don't get it)

Bad use of PCA: prevent overfitting (for it makes features fewer)
not a good way to address overfitting, should use lambda instead
because: it throws away some data without knowing anything about the y

PCA is sometimes used where it shouldn't be:
when designing a ML system, it is not good to design one with PCA at the begining, we should see what if we do without
do PCA only because it is required due to time/memory

https://github.com/vgoklani/Stanford-Machine-Learning/tree/master/k_means-pca/mlclass-ex7



Week 9: 

Lesson 1: Anomaly Detection

Density Estimation:

Problem Motivation: Anomaly Detection is mainly for unsupervised learning, but also has similar aspect with supervised learning
Anomaly Detection example:

- Fraud Detection:
- Dataset {x(1), x(2), ... x(m)}
- Goal x_test
- Is x_test anomaly (very different from the training set)?
- Build a model p(x) // probability
-- if p(x) < ε (epsilon), flag anomaly; if p(x) >= ε, Okay
- useful in finding unusual behaviors

- Manufacturing: send the unusual pieces for further review

- Monitoring Computers in a data center

Gaussian Distribution: also called normal distribution
x~N(μ, σ^2) //"~": distributed as; μ: mu, mean parameter; σ: sigma, variant parameter (σ^2 is called variance), standard deviation
p is written as: p(x; μ, σ^2), means that x is distributed as N(μ, σ^2)
when μ is changed, move left / right; σ larger, then flatter and shorter and wider
parameter estimation problem: estimate (μ, σ^2)
- μ: mean of x(i) // i = 1~m
- σ^2: mean of (x(i) - μ)^2 // i = 1~m; actually in math we should devide it with (m-1) not (m), but they ar both okay with ML

Algorithm: a density estimation algorithm
p(x) = p(x1; μ1, σ1^2) * p(x2; μ2, σ2^2) * ... p(xn; μn, σn^2)
= ∏ (j = 1~n) p(xj; μj, σj^2)

Anomaly Detection Algorithm:
- Choose Feature x that you think might be indicative of anomalous examples
- Fit parameters μ1 ~ μn, σ1^2 ~ σn^2
- Given new examples, compute p(x), anomaly if p(x) < ε

Building an Anomaly Detection System

Developing and Evaluating an Anomaly Detection System: How to evaluate an anomaly setection algorithm
in order to evaluate: assume we have some labeled data, y=0 for non-anomalous and y=1 for anomalous
- Training set, Cross Validation set, Test set
- Flawed data (anomalous) should only be in CV and Test sets
- CV and Test sets shouldn't be the same- that's not good habit
- Fit parameter then run prediction on test set /(or) cross validation set
- Possible evaluation metrics: (always skewed)
-- True / False + Positive / Negative
-- Precision / Recall
-- F1-Score
- Can also use Cross Validation set to choose ε

Anomaly Detection vs. Supervised Learning

Strength fields -
Anomaly Detection:
- Very small number of positive (y = 1) examples (normally 0~20), large number of negative examples (y = 0) used to fit p(x)
- Many different "types" of anomalies, hard for any algorithm to learn from positive examples the common features
- Future anolalies might not look like any of the anomalous we've seen so far
Supervised Learning:
- Large number of positive and negative examples
- Enough positive examples for algorithms to learn what positive examples are like
- Further positive examples tend to be like those in the training set

Choosing What Features to Use:
p(x) work okay if it's not Gaussian
// hist - Octave
play with transimation to make non-Gaussian features Gaussian
e.g. log(x1), log(x2 + 2), x3^(1/2), x4^(1/3)
- Error Analysis for Anomaly Detection: want normal and anomalous examples have distinguishable p(x)
- Choose features that might take on unusually large or small values in the event of anomaly
-- maybe create x5 = x1 / x2 or so to detect unusual combinations

Multivariate Gaussian Distribution (Optional)

Multivariate Gaussian Distribution: extension
better in discover combined-features' anomaly
don't model p(x1), p(x2) ... separately, model p(x) all in one go
parameters: μ ∈ R^n, Σ ∈ R^(n*n) (covariance matrix)
p(x; μ, Σ)
|Σ|: determinet of Σ, Octave: det(sigma)
// on the diag of Σ, each value represent the width of the "mount"
// outside the diag of Σ (off-diag), values represent the correlation between parameters (the shape of the "mount" is twistted)

Anomaly Detection using the Multivariate Gaussian Distribution
μ: the average ( / m) of training examples
Σ: Σ = (1/m) * ∑(i=1~m) (x(i) - μ) * (x(i) - μ)^T
when off-diag Σ elements are all 0; multi-Gaussian could be represented by p(x1) * p(x2) * ...
when do we use:
Original Model:
- Manually create features to capture anomalies where x1, x2 take unusual combinations of values
- Coputationally cheaper, scale better to large n
- Okay even if m is small
Multivariate Gaussian:
- Automatically captures correlations between features
- Expensive to compute
- must have m > n, or else Σ would be non-invertible (could have had highly redundant variables, reduce them might solve the problem)


Lesson 2: Recommender Systems

Predicting Movie Ratings:

Content Based Recommendations
n_u: users
n_m: movies
n_m * n_u: rating (score)
n_m * (x1, x2): x1, x2 - the degree to which the movie is romance (x1) / action (x2); this is the vector of x
for each user j, learn a parameter Θ(j) (if x0=1, x1, x2 then R3), predict user j as rating movie i with (Θ(j))^T * x(i)

Problem formulation:
r(i, j) = 1: if user j has rated movie i, otherwise = 0
y(i, j): rating by user j on movie i (if defined)
Θ(j): parameter vector of user j
x(i): feature vector of movie i
For movie i, user j predicting rate: (Θ(j))^T * (x(i))
To learn Θ(j):
- m(j): the amount of movies user j rated
- min(Θ(j)) [(1/(2*m(j))) * ∑(i:r(i, j)=1) ((Θ(j))^T * (x(i)) - y(i, j))^2 + λ/(2*m(j)) * ∑(k =1~n)Θk(j)^2]
- min(Θ(j)) [1/2 * ∑(i:r(i, j)=1) ((Θ(j))^T * (x(i)) - y(i, j))^2 + λ/2 * ∑(k =1~n)Θk(j)^2]
- learn Θ(1), ... Θ(n_u):
-- min(Θ(1), ... Θ(n_u))[1/2 * ∑(j = 1~n_u) ∑(i:r(i, j)=1) ((Θ(j))^T * (x(i)) - y(i, j))^2 + λ/2 * ∑(j = 1~n_u) ∑(k =1~n)Θk(j)^2]
- Optimization Algorithm:
-- Θk(j) := Θk(j) - α * (∑(i:r(i, j)=1) ((Θ(j))^T * (x(i)) - y(i, j)) * x_k(i)) // for k = 0
-- Θk(j) := Θk(j) - α * (∑(i:r(i, j)=1) ((Θ(j))^T * (x(i)) - y(i, j)) * x_k(i) + λ * Θk(j)) // for k != 0

Collaborative Filtering:

could do "feature learning": learn features itself
Θ(j): how does user j like different kind of movies (according to features, not specific movies)
x(i): the feature vector of movie i
Θ(j)^T * x(i): user j, movie i rating

Optimization Algorithm:
learn x from Θ
- Given Θ(1) ... Θ(n_u) to learn x(i):
-- min(x(i)) [1/2 * ∑(j:r(i, j)=1) ((Θ(j))^T * (x(i)) - y(i, j))^2 + λ/2 * ∑(k =1~n)x_k(i)^2]
- Given Θ(1) ... Θ(n_u) to learn x(1) ... x(n_m):
-- min(x(1), ... x(n_m))[1/2 * ∑(i = 1~n_m) ∑(j:r(i, j)=1) ((Θ(j))^T * (x(i)) - y(i, j))^2 + λ/2 * ∑(i = 1~n_m) ∑(k =1~n)x_k(i)^2]
learn Θ from x
- Given x(1) ... x(n_m) to learn Θ(j):
-- min(Θ(j)) [1/2 * ∑(i:r(i, j)=1) ((Θ(j))^T * (x(i)) - y(i, j))^2 + λ/2 * ∑(k =1~n)Θ_k(j)^2]
- learn Θ(1), ... Θ(n_u):
-- min(Θ(1), ... Θ(n_u))[1/2 * ∑(j = 1~n_u) ∑(i:r(i, j)=1) ((Θ(j))^T * (x(i)) - y(i, j))^2 + λ/2 * ∑(j = 1~n_u) ∑(k =1~n)Θk(j)^2]
Initial:
- Guess Θ
- then learn x
- then learn Θ
- ...

Collaborative Filtering Algorithm:
Minimizing Θ and x simultaneously:
- J(x(1), ... x(n_m), Θ(1), ... Θ(n_u)) = 1/2 * ∑((i, j):r(i, j)=1) ((Θ(j))^T * (x(i)) - y(i, j))^2 + λ/2 * ∑(i = 1~n_m) ∑(k =1~n)x_k(i)^2] + λ/2 * ∑(j = 1~n_u) ∑(k =1~n)Θk(j)^2]
- min(x(1), ... x(n_m)) (Θ(1), ... Θ(n_u)) J
- Previously x is R(n + 1) for x0 = 1; now we could get rid of it, and x is R(n) // because no need to hard code a 1; it could learn it self if necessary
Initial:
- all x and Θ elements to be: small random values, x and Θ are both R(n)
- serve as symmetry breaking, ensures the x(1) ... x(n_m) are different from each other
Minimize using gradient descent: (or advanced optimization algorithm)
- x_k(i) := x_k(i) - α * (∑(j:r(i, j)=1) ((Θ(j))^T * (x(i)) - y(i, j)) * Θk(j) + λ * x_k(i))
- Θk(j) := Θk(j) - α * (∑(i:r(i, j)=1) ((Θ(j))^T * (x(i)) - y(i, j)) * x_k(i) + λ * Θk(j))
Predict:
- user with Θ and movie with x: Θ^T * x

Vectorization: Low Rank Matrix Factorization

vectorization implementation of Collaborative Filtering
movies: n_m
users: n_u
Y: users' ratings, a n_m * n_4 matrix; predict(j, i) = (Θ(j))^T * (x(i))
X: [x(1);x(2);...x(n_m)]
Θ: [Θ(1);Θ(2);...Θ(n_u)]
Predict = X* Θ^T

Something else you can do with CF: finding related movies
- learn features
- recommend movie j according to i
- small ||x(j) - x(i)|| means similar

Implementational Detail: Mean Normalization:
- Users who has not yet rated any movies
-- μ = [a(1); a(2); ... a(n_m)] // a(i) is the average rating of movie i
-- for each Y(j, i), change it to be Y(j, i) - a(j), so average is zero
-- learn Θ(j) and x(i) according to it
-- Predict (Θ(j))^T * (x(i)) + μ_i
-- For a new user: predict = μ_i
- New Movies:
-- also do as the similar (viewed as average)
- Already on the same scale

https://github.com/vgoklani/Stanford-Machine-Learning
https://github.com/vgoklani/Stanford-Machine-Learning/tree/master/anomaly_det-recommender/mlclass-ex8



Week 10: Large Scale Machine Learning

Gradient Descent with Large Datasets

Learning With Large Datasets:
- Data is booming these days
- Larger dataset always makes the performance better
- Computational problem: large dataset => derivative computed really slow => replace / find efficient way

How to know if it'll be better with a large dataset? High variance when m is small. - By plotting J_cv && J_train - m; if the two lines are very different from each other, increasing the data amount would help a lot

Stochastic Gradient Descent & Map Reduce

Stochastic Gradient Descent: a modification to the traditional Gradient Descent Algorithm
Traditional GD: Batch Gradient Descent
- Why called Batch: we're looking at all the training examples at a time
- Stochastic Gradient Descent: only one training example at a time
cost(Θ, (x(i), y(i))) = 1/2 * (hΘ(x(i)) - y(i))^2
J_train(Θ) = 1/m * ∑(i=1~m) cost(Θ, (x(i), y(i)))
- 1. randomly shuffle (re-write) dataset
- 2. repeat: // Batch: Θj := Θj - α/m * ∑ (i = 1~m) (hΘ(x(i)) - y(i)) * x_j(i)
---- for i = 1~m, {
-------- for j = 0~n, {
------------ Θj := Θj - α * (hΘ(x(i)) - y(i)) * x_j(i) 
-------- }
---- }
- Fit the i-th example a little bit better at the i-th iteration
- Faster & Safer
- Could help explain why start by randomly shaffling the dataset
- Generally move to the global minimum J but not always
- Not converging like Batch; it wanders around
- Repeat the inner loop of "repeat" for probably 1 to 10 times (passes)

Mini-Batch Gradient Descent: might be even faster
use b examples in each iteration (instead of m, but not 1)
"look for a better solution for the b examples"
vectorization makes it possible for it to perform better than SGD
Becomes Batch when b = m, becomes SGD when b = 1

Stochastic Gradient Descent Convergence

making sure the convergence:
Batch: plot J-iteration, make sure it is decreasing
SGD: compute J would need to compute entire m data, so: compute cost(Θ, (x(i), y(i))) = 1/2 * (hΘ(x(i)) - y(i))^2 before using (x(i), y(i)) to update Θ
- Every k iteration, plot the cost(Θ, (x(i), y(i))) averaged over the k examples
- a smaller learning rate: go down more slowly, but might end up in a slightly better solution
- increasing k might make it smoother; a small k might means it would be too noisy
- If it is diverging: decrease alpha!!! (learning rate)
- could decreaase alpha over time if we want to converge to the global minimal (const1 / (const2+iterations))
- use this method to: 1) make sure it is converging; 2) tune learning rate alpha

Advanced Topics

Online Learning
update Θ with new coming (x, y) // one at a time
after learning from an example we discard it
- slowly adapt to different pool of users
- suitable for a continuous stream of data
- do require selecting alpha & good features chosen

Map Reduce and Data Parallelism
Map reduce is really important idea; at leat as important as GD
Map-reduce could be applied to Batch GD, but not SGD
- the idea was from Google
- seperately compute & unite them together
- apply to:
-- multiple computers (clusters & data center)
-- multi-core machines (multiple CPU / multiple core) // no network latency~
could work for neuro network - each compute back for 1/n and forward for 1/n



Week 11: Application Example: Photo OCR

Problem Description and Pipeline

Photo OCR=Photo Optical Character Recognition
focusing on the text in the image: still a hard problem today (from photo; but from scanned document, that's easy)

Photo OCR pipeline:
1. Text detection - find the region where the text is in
2. Character segmentation - devide the region into characters
3. Character classification - get the exact character
(maybe more complex, e.g. spell correcter)
this is a pipeline; it is common; have multiple modules; some might be ML components, some are not

Sliding Windows:
Sliding Window Detector is used for text detection
-pedestrian detection: rectangle with fixed width and height ratio, w/h fixed and positive/negative examples labelled, slide over a bit (step size / stride) in a -image at a time, use different size of windows to detect; 
-but text areas' h/w vary
-Text detection: similar with pedestrian - regions of text (y = 1 examples), regions without (y = 0)
-find text by first spot characters => "expansion operator"
Sliding Window Detector is also used for character segmentation
-supervised learning algorithm again (y = 1/0); y=1 stands for "spot the midpoint between two characters"

Getting Lots of Data and Artificial Data
- get a low-bias algorithm and train it on lots of data would make a good algorithm
- where are data from?
- artificial data synthesis
- method 1: use digital fonts + random background => create data
- method 2: take the existing data and create new data => artificial distortion; amplify; should represent a certain type of noise, otherwise unlikely to help

Getting more data:
1. before: make sure "low bias" condition (plot learning curve)
2. how much work would it be if we get more data?
	- artificial data synthesis (make/distort)
	- collect / label it yourself (somebody would have to spend certain amount of time - calculate the time amount first!)
	- "Crowd Source" (e.g. Amazon Mechanical Turk)

Ceiling Analysis: What Part of the Pipeline to Work on Next
find the critical part of the project
what part of the pipeline best worth work on
list component - accuracy (overall system; module 1 (text detection); module 2; ... etc.)
how to get the accuracy of each step? => use 100% accurate data (might be mannually inputed) to move on to the next step
if it worth:
module 1 improvement raise: accuracy(Module 1) - accuracy(Overall System)
module 2 improvement raise: accuracy(Module 2) - accuracy(Module 1)
...
etc.
potential increase: at most how much could be improved if we improve it (and all stages before it) to 100% correct

http://octave.sourceforge.net/octave/function/ones.html

